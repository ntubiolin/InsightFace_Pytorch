{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T12:16:01.581503Z",
     "start_time": "2018-08-06T12:16:01.377552Z"
    }
   },
   "outputs": [],
   "source": [
    "# # see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "# import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T12:16:02.981513Z",
     "start_time": "2018-08-06T12:16:01.884463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import time\n",
    "import argparse\n",
    "from glob import glob\n",
    "from data.data_pipe import get_val_pair\n",
    "from torchvision import transforms as trans\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from IPython.display import display\n",
    "\n",
    "from Learner import face_learner\n",
    "from config import get_config\n",
    "from utils import hflip_batch, cosineDim1\n",
    "from data.datasets import (IJBVerificationPathDataset, IJBAVerificationDataset)\n",
    "from verification import calculate_val_by_diff, calculate_val_far\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Learner and conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detachAttentionGradient': True, 'fixed_str': 'ir_se50.pth', 'pretrainedMdl': 'ir_se50.pth', 'data_path': PosixPath('data'), 'work_path': PosixPath('work_space'), 'model_path': PosixPath('work_space/models'), 'log_path': PosixPath('work_space/log'), 'save_path': PosixPath('work_space/save'), 'exp_title': 'xCos', 'exp_comment': 'fromScratch_CosFace', 'input_size': [112, 112], 'embedding_size': 1568, 'use_mobilfacenet': False, 'modelType': 'ArcFace', 'net_depth': 50, 'drop_ratio': 0.6, 'net_mode': 'ir_se', 'device': device(type='cuda', index=0), 'test_transform': Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "), 'data_mode': 'emore', 'vgg_folder': PosixPath('data/faces_vgg_112x112'), 'ms1m_folder': PosixPath('data/faces_ms1m_112x112'), 'emore_folder': PosixPath('data/faces_emore'), 'batch_size': 20, 'USE_SOFTMAX': True, 'SOFTMAX_T': 1, 'facebank_path': PosixPath('data/facebank'), 'threshold': 1.5, 'threshold_xCos': 0.2338, 'face_limit': 10, 'min_face_size': 30}\n",
      "ir_se_50 model generated\n"
     ]
    }
   ],
   "source": [
    "conf = get_config(training=False)\n",
    "conf.batch_size=20 # Why bs_size can only be the number that divide 6000 well?\n",
    "learner = face_learner(conf, inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2normalize(x, ord=2, axis=None):\n",
    "    return x / np.linalg.norm(x, ord=ord, axis=axis)\n",
    "\n",
    "\n",
    "def score_fn_original(feat1, feat2):\n",
    "    return l2normalize(feat1).dot(l2normalize(feat2))\n",
    "    \n",
    "\n",
    "def score_fn_ours(feat_map1, feat_map2, learner, attention_strategy='learned', attention_weight=None):\n",
    "    with torch.no_grad():\n",
    "        assert len(feat_map1.shape) == 3  # [c, w, h]\n",
    "        c, w, h = feat_map1.shape\n",
    "        feat_map1 = torch.tensor(feat_map1).unsqueeze(0).to(conf.device)\n",
    "        feat_map2 = torch.tensor(feat_map2).unsqueeze(0).to(conf.device)\n",
    "        if attention_strategy == 'learned':\n",
    "            attention = None\n",
    "        elif attention_strategy == 'uniform':\n",
    "            learner.model_attention.eval()\n",
    "            attention = torch.ones([1, 1, w, h])  # [batch, c, w, h]\n",
    "            attention /= attention.sum()\n",
    "            attention = attention.to(conf.device)\n",
    "        elif attention_strategy == 'fixed':\n",
    "            assert attention_weight is not None\n",
    "            assert attention_weight.shape[0] == w and attention_weight.shape[1] == h\n",
    "            attention = torch.tensor(attention_weight).view(\n",
    "                [1, 1, attention_weight.shape[0], attention_weight.shape[1]]\n",
    "            ).type(feat_map1.type())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        xCos, attention, cos_patched = learner.get_x_cosine(feat_map1, feat_map2, attention)\n",
    "    return xCos.cpu().numpy()\n",
    "\n",
    "def evaluate_and_plot(scores, is_sames, logger, nrof_folds=10, \n",
    "                      dataset_name='AR Face', display_roc_curve=True):\n",
    "    accuracy, best_threshold, roc_curve_tensor = learner.evaluate_and_plot_roc(scores, is_sames, nrof_folds=10)\n",
    "    message = f'{dataset_name} - accuray:{accuracy:.5f} (1-{1-accuracy:.5f}), threshold:{best_threshold}'\n",
    "    print(message)\n",
    "    logger.info(message)\n",
    "    #with open(log_filename, 'a') as f:\n",
    "    #    f.write(message + '\\n')\n",
    "    if display_roc_curve:\n",
    "        roc_img = trans.ToPILImage()(roc_curve_tensor)\n",
    "        display(roc_img)\n",
    "        # roc_img.save(log_filename[:-4] + '.png')\n",
    "        roc_img.save(logger.handlers[0].baseFilename[:-4] + '.png')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on IJB-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8010270"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IJB_B_or_C = 'IJBB' # 'IJBC'\n",
    "leave_ratio = 1\n",
    "ijb_feat_root=f'work_space/IJB_features/{IJB_B_or_C}_ir_se50/',\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    IJBVerificationPathDataset(ijb_dataset_root=f'data/IJB_release/{IJB_B_or_C}/', \n",
    "                               dataset_type=IJB_B_or_C,\n",
    "                               leave_ratio=leave_ratio),\n",
    "    batch_size=1, shuffle=False\n",
    ")\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_feature_original(feat_dir, suffixes, ends_with='.npy'):\n",
    "    return np.concatenate([\n",
    "        np.load(op.join(feat_dir, suffix[0]) + ends_with)\n",
    "        for suffix in suffixes\n",
    "    ], axis=0)\n",
    "\n",
    "def _get_feature_ours(feat_dir, suffixes, ends_with='.npz'):\n",
    "#     print(op.join(feat_dir, suffixes[0][0]) + ends_with)\n",
    "    return np.concatenate([\n",
    "        np.load(op.join(feat_dir, suffix[0]) + ends_with)['feat_map']\n",
    "        for suffix in suffixes\n",
    "    ], axis=0)\n",
    "\n",
    "\n",
    "def run_IJBC_verification(loader, feat_dir, compare_strategy, _get_feature, \n",
    "                          score_fn, score_fn_kargs,\n",
    "                          logger,\n",
    "                          learner=None, attention_strategy=None):\n",
    "    assert compare_strategy in ['compare_only_first_img', 'mean_comparison']\n",
    "    is_sames = []\n",
    "    scores = []\n",
    "    init_time = time.time()\n",
    "    ignored_count = 0\n",
    "    tmp_same_count = 0\n",
    "    # with open(log_filename, 'w') as f:\n",
    "    msg_total = f\"total number: {len(loader)}\"\n",
    "    print(msg_total)\n",
    "    # f.write(msg_total + '\\n')\n",
    "    logger.info(msg_total)\n",
    "    for i, pair in enumerate(loader):\n",
    "        if i % 10000 == 0:\n",
    "            msg = (f\"Processing match {i}, elapsed {time.time() - init_time:.1f}\\\n",
    "            seconds, positive ratio: {tmp_same_count / (i+1):.4f}\")\n",
    "            print(msg)\n",
    "            # f.write(msg + '\\n')\n",
    "            logger.info(msg)\n",
    "\n",
    "        if len(pair['enroll_path_suffixes']) == 0 or len(pair['verif_path_suffixes']) == 0:\n",
    "            ignored_count += 1\n",
    "            continue\n",
    "\n",
    "        if compare_strategy == 'compare_only_first_img':\n",
    "            enroll_feature = _get_feature(feat_dir, pair['enroll_path_suffixes'][:1]).squeeze(0)\n",
    "            verif_feature = _get_feature(feat_dir, pair['verif_path_suffixes'][:1]).squeeze(0)\n",
    "        elif compare_strategy == 'mean_comparison':\n",
    "            print('Warning: using mean_comparison')\n",
    "            raise NotImplementedError\n",
    "            enroll_feature = _get_feature(feat_dir, pair['enroll_path_suffixes']).mean(axis=0)\n",
    "            verif_feature = _get_feature(feat_dir, pair['verif_path_suffixes']).mean(axis=0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        score = score_fn(enroll_feature, verif_feature, **score_fn_kargs)\n",
    "        scores.append(score)\n",
    "        # XXX: Why is pair['is_same'] a list?\n",
    "        # if bool(int(pair['is_same'][0])):\n",
    "        #     print((int(pair['is_same'][0])))\n",
    "        #     print('    >>>', bool(int(pair['is_same'][0])))\n",
    "        #     print(pair)\n",
    "        is_same_label = bool(int(pair['is_same'][0]))\n",
    "        is_sames.append(is_same_label)\n",
    "        if is_same_label:\n",
    "            tmp_same_count += 1\n",
    "    msg_ignored = f'{ignored_count} pairs are ignored since one of the template has no valid image.'\n",
    "    print(msg_ignored)\n",
    "    #f.write(msg_ignored + '\\n')\n",
    "    logger.info(msg_ignored)\n",
    "    ############\n",
    "    scores = np.array(scores).squeeze()\n",
    "    is_sames = np.array(is_sames).squeeze().astype(np.bool)\n",
    "    np.savetxt(f\"{logger.handlers[0].baseFilename[:-4]}_scores.csv\", scores, delimiter=\",\")\n",
    "    np.savetxt(f\"{logger.handlers[0].baseFilename[:-4]}_is_sames.csv\", is_sames, delimiter=\",\")\n",
    "    return scores, is_sames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "all_acc = {}\n",
    "\n",
    "def record_scores_and_acc(scores, acc, name, name2=None):\n",
    "    all_scores[name] = scores\n",
    "    all_acc[name] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run irse-50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number: 8010270\n",
      "Processing match 0, elapsed 0.0 seconds, positive ratio: 0.0000\n",
      "Processing match 10000, elapsed 13.4 seconds, positive ratio: 0.9999\n",
      "Processing match 20000, elapsed 26.9 seconds, positive ratio: 0.5135\n",
      "Processing match 30000, elapsed 40.3 seconds, positive ratio: 0.3423\n",
      "Processing match 40000, elapsed 53.7 seconds, positive ratio: 0.2567\n",
      "Processing match 50000, elapsed 67.7 seconds, positive ratio: 0.2054\n",
      "Processing match 60000, elapsed 81.0 seconds, positive ratio: 0.1712\n",
      "Processing match 70000, elapsed 94.7 seconds, positive ratio: 0.1467\n",
      "Processing match 80000, elapsed 108.6 seconds, positive ratio: 0.1284\n",
      "Processing match 90000, elapsed 122.5 seconds, positive ratio: 0.1141\n",
      "Processing match 100000, elapsed 136.1 seconds, positive ratio: 0.1027\n",
      "Processing match 110000, elapsed 149.2 seconds, positive ratio: 0.0934\n",
      "Processing match 120000, elapsed 162.6 seconds, positive ratio: 0.0856\n",
      "Processing match 130000, elapsed 176.3 seconds, positive ratio: 0.0790\n",
      "Processing match 140000, elapsed 190.2 seconds, positive ratio: 0.0734\n",
      "Processing match 150000, elapsed 203.7 seconds, positive ratio: 0.0685\n",
      "Processing match 160000, elapsed 217.4 seconds, positive ratio: 0.0642\n",
      "Processing match 170000, elapsed 231.3 seconds, positive ratio: 0.0604\n",
      "Processing match 180000, elapsed 244.6 seconds, positive ratio: 0.0571\n",
      "Processing match 190000, elapsed 258.0 seconds, positive ratio: 0.0541\n",
      "Processing match 200000, elapsed 271.6 seconds, positive ratio: 0.0513\n",
      "Processing match 210000, elapsed 285.3 seconds, positive ratio: 0.0489\n",
      "Processing match 220000, elapsed 298.6 seconds, positive ratio: 0.0467\n",
      "Processing match 230000, elapsed 311.7 seconds, positive ratio: 0.0447\n",
      "Processing match 240000, elapsed 325.2 seconds, positive ratio: 0.0428\n",
      "Processing match 250000, elapsed 338.5 seconds, positive ratio: 0.0411\n",
      "Processing match 260000, elapsed 352.1 seconds, positive ratio: 0.0395\n",
      "Processing match 270000, elapsed 365.4 seconds, positive ratio: 0.0380\n",
      "Processing match 280000, elapsed 378.8 seconds, positive ratio: 0.0367\n",
      "Processing match 290000, elapsed 392.2 seconds, positive ratio: 0.0354\n",
      "Processing match 300000, elapsed 405.8 seconds, positive ratio: 0.0342\n"
     ]
    }
   ],
   "source": [
    "model_name = f'{IJB_B_or_C}_ir_se50'\n",
    "log_filename = f\"logs/Log_{model_name}.txt\"\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "scores, is_sames = run_IJBC_verification(\n",
    "    loader, feat_dir=f'work_space/IJB_features/{model_name}/loose_crop/',\n",
    "    score_fn=score_fn_original, _get_feature=_get_feature_original,\n",
    "    compare_strategy='compare_only_first_img', score_fn_kargs={},\n",
    "    logger=logger,\n",
    ")\n",
    "acc = evaluate_and_plot(scores, is_sames, nrof_folds=10, dataset_name=model_name, \n",
    "                        logger=logger)\n",
    "all_scores['original_ArcFace'] = scores\n",
    "all_acc['original_ArcFace'] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CosFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = '2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace'\n",
    "fixed_weight = np.load(f'data/correlation_weights/{model_name}.npy')\n",
    "fixed_weight /= fixed_weight.sum()\n",
    "\n",
    "for attention_strategy in ['uniform', 'learned', 'fixed']:\n",
    "    print(f'===== attention_strategy: {attention_strategy} =====')\n",
    "    log_filename = f\"logs/Log_{IJB_B_or_C}_{model_name}_{attention_strategy}.txt\"\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=log_filename, level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    scores, is_sames = run_IJBC_verification(\n",
    "        loader, feat_dir=f\"work_space/IJB_features/{IJB_B_or_C}_{model_name}/loose_crop\",\n",
    "        score_fn=score_fn_ours, _get_feature=_get_feature_ours,\n",
    "        compare_strategy='compare_only_first_img',\n",
    "        score_fn_kargs={'learner': learner, 'attention_strategy': attention_strategy,\n",
    "                        'attention_weight': fixed_weight},\n",
    "        logger = logger\n",
    "    )\n",
    "    acc = evaluate_and_plot(scores, is_sames, nrof_folds=10, dataset_name=f\"{IJB_B_or_C}_{model_name}\",\n",
    "                            logger=logger)\n",
    "\n",
    "    all_scores[f'{model_name}_{attention_strategy}'] = scores\n",
    "    all_acc[f'{model_name}_{attention_strategy}'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ArcFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '2019-08-30-07-36_accuracy:0.9953333333333333_step:655047_None'\n",
    "fixed_weight = np.load(f'data/correlation_weights/{model_name}.npy')\n",
    "fixed_weight /= fixed_weight.sum()\n",
    "\n",
    "for attention_strategy in ['uniform', 'learned', 'fixed']:\n",
    "    print(f'===== attention_strategy: {attention_strategy} =====')\n",
    "    log_filename = f\"logs/Log_{IJB_B_or_C}_{model_name}_{attention_strategy}.txt\"\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=log_filename, level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    scores, is_sames = run_IJBC_verification(\n",
    "        loader, feat_dir=f\"work_space/IJB_features/{IJB_B_or_C}_{model_name}/loose_crop\",\n",
    "        score_fn=score_fn_ours, _get_feature=_get_feature_ours,\n",
    "        compare_strategy='compare_only_first_img',\n",
    "        score_fn_kargs={'learner': learner, 'attention_strategy': attention_strategy,\n",
    "                        'attention_weight': fixed_weight},\n",
    "        logger = logger\n",
    "    )\n",
    "    acc = evaluate_and_plot(scores, is_sames, nrof_folds=10, dataset_name=f\"{IJB_B_or_C}_{model_name}\",\n",
    "                            logger=logger)\n",
    "\n",
    "    all_scores[f'{model_name}_{attention_strategy}'] = scores\n",
    "    all_acc[f'{model_name}_{attention_strategy}'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IJB-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "all_acc = {}\n",
    "\n",
    "def record_scores_and_acc(scores, acc, name, name2=None):\n",
    "    all_scores[name] = scores\n",
    "    all_acc[name] = acc\n",
    "    \n",
    "shuffle_order = np.arange(len(IJBAVerificationDataset()))\n",
    "np.random.shuffle(shuffle_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For IJB-A, I save features of t1 and t2 \"for each comparison\" and the corresponding label (is same or not)\n",
    "'''\n",
    "def _get_feature(fname):\n",
    "    npz = np.load(fname)\n",
    "    return npz['f1'], npz['f2'], npz['same']\n",
    "\n",
    "def run_IJBA_verification(feat_dir, score_fn, score_fn_kargs, shuffle_order,\n",
    "                          learner=None, attention_strategy=None, ):\n",
    "    is_sames = []\n",
    "    scores = []\n",
    "    init_time = time.time()\n",
    "    fnames = sorted(glob(op.join(feat_dir, '*.npz')))\n",
    "    fnames = [fnames[i] for i in shuffle_order]\n",
    "    print(len(fnames))\n",
    "    for i, fname in enumerate(fnames):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Processing match {i}, elapsed {time.time() - init_time:.1f} seconds\")\n",
    "        f1, f2, is_same = _get_feature(fname)\n",
    "        score = score_fn(f1, f2, **score_fn_kargs)\n",
    "        score = score.cpu().numpy() if torch.is_tensor(score) else score\n",
    "        scores.append(score)\n",
    "        is_sames.append(is_same.astype(np.bool))\n",
    "    \n",
    "    scores = np.array(scores).squeeze()\n",
    "    is_sames = np.array(is_sames).squeeze().astype(np.bool)\n",
    "    return scores, is_sames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, is_sames = run_IJBA_verification(\n",
    "    feat_dir='./saved_features/IJB-A/split1/ir_se50/',\n",
    "    score_fn=score_fn_original, score_fn_kargs={},\n",
    "    shuffle_order=shuffle_order\n",
    ")\n",
    "print(np.histogram(scores), np.histogram(is_sames))\n",
    "acc = evaluate_and_plot(scores, is_sames, nrof_folds=10, dataset_name='IJBC')\n",
    "all_scores['original_ArcFace'] = scores\n",
    "all_acc['original_ArcFace'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for far_target in [1e-1, 1e-2, 1e-3]:\n",
    "    mean_tar = calculate_val_by_diff(\n",
    "        thresholds='default', dist=(1 - scores), actual_issame=is_sames, far_target=0.001, ret_acc=True,\n",
    "        nrof_folds=5\n",
    "    )[0]\n",
    "    print(\"TAR@FAR{far_target:.f}:{mean_tar}\")\n",
    "# calculate_val_far()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_plot_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '2019-09-02-08-21_accuracy:0.9968333333333333_step:436692_CosFace'\n",
    "fixed_weight = np.load(f'/tmp3/biolin/data/insightFace_pytorch/{model_name}.npy')\n",
    "fixed_weight /= fixed_weight.sum()\n",
    "\n",
    "for attention_strategy in ['uniform', 'learned', 'fixed']:\n",
    "    print(f'===== attention_strategy: {attention_strategy} =====')\n",
    "    scores, is_sames = run_IJBA_verification(\n",
    "        feat_dir=f'./saved_features/IJB-A/split1/{model_name}',\n",
    "        score_fn=score_fn_ours,\n",
    "        score_fn_kargs={'learner': learner, 'attention_strategy': attention_strategy,\n",
    "                        'attention_weight': fixed_weight},\n",
    "        shuffle_order=shuffle_order\n",
    "    )\n",
    "    acc = evaluate_and_plot(scores, is_sames, nrof_folds=10, dataset_name='IJBC')\n",
    "\n",
    "    all_scores[f'{model_name}_{attention_strategy}'] = scores\n",
    "    all_acc[f'{model_name}_{attention_strategy}'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following codes are just testing\n",
    "## Evaluate on LFW (just testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUnitAttention():\n",
    "    x = torch.ones(conf.batch_size//2, 1, 7, 7).cuda()\n",
    "    x /= x.flatten(2).sum(dim=2).repeat(1, 1, x.size(2) * x.size(3)).view_as(x)\n",
    "    return x\n",
    "unit_attention = getUnitAttention()\n",
    "unit_attention.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw, lfw_issame = get_val_pair(conf.emore_folder, 'lfw')\n",
    "learner.load_state(conf, 'ir_se50.pth', model_only=True, from_save_folder=True, strict=False, model_atten=False)\n",
    "learner.model.eval()\n",
    "learner.model.returnGrid = True  # Remember to reset this before return!\n",
    "learner.model_attention.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our models (wo/w attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# accuracy, best_threshold, roc_curve_tensor = learner.evaluate_attention(\n",
    "#     conf, lfw, lfw_issame, nrof_folds=10, tta=True, attention=unit_attention\n",
    "# )\n",
    "# print('lfw - accuray:{}, threshold:{}'.format(accuracy, best_threshold))\n",
    "# trans.ToPILImage()(roc_curve_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# accuracy, best_threshold, roc_curve_tensor = learner.evaluate_attention(\n",
    "#     conf, lfw, lfw_issame, nrof_folds=10, tta=True, attention=None\n",
    "# )\n",
    "# print('lfw - accuray:{}, threshold:{}'.format(accuracy, best_threshold))\n",
    "# trans.ToPILImage()(roc_curve_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learner.load_state(conf, 'ir_se50.pth', model_only=True, from_save_folder=True, strict=False, model_atten=False)\n",
    "cosines = learner.get_original_cosines(lfw, conf, tta=True)\n",
    "accuracy, best_threshold, roc_curve_tensor = learner.evaluate_and_plot_roc(cosines, lfw_issame, nrof_folds=10)\n",
    "print('lfw - accuray:{}, threshold:{}'.format(accuracy, best_threshold))\n",
    "trans.ToPILImage()(roc_curve_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_val_by_diff(\n",
    "    thresholds='default', dist=(1 - cosines), actual_issame=lfw_issame,\n",
    "    far_target=0.001, ret_acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_val_far((1 - 0.28250000000000103), (1 - cosines), lfw_issame, ret_acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
